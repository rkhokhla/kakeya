\documentclass[11pt]{article}

% arXiv-compatible packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage[margin=1in]{geometry}
\usepackage{caption}
\usepackage{subcaption}

% Theorem environments
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}{Definition}

% Hyperref setup
\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  citecolor=blue,
  urlcolor=blue
}

\title{Ensemble Verification for LLM Output Quality Assessment:\\
\textbf{Lessons from the Synthetic-to-Production Gap}}

\author{
  Roman Khokhla\\
  Independent Researcher\\
  \texttt{rkhokhla@gmail.com}
}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
The discovery that compressibility-based signals achieve perfect detection (AUROC 1.000) on synthetic degeneracy but flag high-quality outputs on production models (GPT-4) reveals a fundamental challenge: \textbf{different failure modes require different signals}. We investigate whether ensemble approaches combining geometric signals ($\hat{D}$ fractal dimension, $\operatorname{coh}_\star$ coherence, $r_{\text{LZ}}$ compressibility) with perplexity improve factual hallucination detection.

Through rigorous analysis of 7,738 labeled GPT-4 outputs from three benchmarks (HaluBench, FEVER, HaluEval), we find an \textbf{honest negative result}: Despite testing 13 feature combinations, geometric signals do NOT significantly improve over perplexity baseline. Key findings: (1) Best single signal $\operatorname{coh}_\star$ (AUROC 0.520) marginally outperforms perplexity (0.503), $r_{\text{LZ}}$ (0.507), and $\hat{D}$ (0.496). Full ensemble achieves 0.540 (+7.4\%). (2) McNemar's tests show NO significant differences (all $p > 0.05$). Full ensemble vs baseline: $p=0.499$. (3) Task mismatch: geometric signals detect structural pathology; factual hallucinations require semantic verification. (4) Production reality: GPT-4 avoids structural degeneracy; signals achieving AUROC 1.000 on synthetic benchmarks perform near random ($\approx 0.50$) on production.

This honest negative result validates the synthetic-production gap and demonstrates that ensemble methods combining geometric signals with perplexity are NOT effective for factual hallucination detection on well-trained models.
\end{abstract}

\section{Motivation: Why Ensemble Approaches?}
\label{sec:motivation}

\subsection{The Multi-Modal Nature of LLM Failures}

LLM outputs can fail in fundamentally different ways:
\begin{itemize}
\item \textbf{Factual errors}: Incorrect claims, false information, contradicting known facts
\item \textbf{Structural pathology}: Repetitive loops, semantic drift, incoherence
\item \textbf{Quality degradation}: Poor lexical variety, simplistic language, hedging
\end{itemize}

Each failure mode has distinct signatures requiring specialized detection:
\begin{itemize}
\item \textbf{Factual errors} $\rightarrow$ Perplexity, NLI entailment, retrieval-augmented verification
\item \textbf{Structural pathology} $\rightarrow$ Compression ratio ($r_{\text{LZ}}$), repetition detection
\item \textbf{Quality markers} $\rightarrow$ Lexical diversity, coherence metrics
\end{itemize}

\subsection{The Synthetic-Production Gap Challenge}

Our previous work~\cite{khokhla2025synthetic} discovered that:
\begin{itemize}
\item Compressibility signal ($r_{\text{LZ}}$) achieves \textbf{AUROC 1.000} on synthetic degeneracy
\item Same signal on 8,290 real GPT-4 outputs flags \textbf{high-quality} responses (inverse enrichment)
\item Outliers exhibit \textbf{higher} lexical diversity (0.932 vs 0.842, Cohen's $d=0.90$)
\item Outliers exhibit \textbf{lower} sentence repetition (0.183 vs 0.274, Cohen's $d=-0.47$)
\end{itemize}

\textbf{Interpretation}: Modern production models (GPT-4) are trained so well they don't produce the structural pathologies that synthetic benchmarks assume. Geometric signals detect what compresses---but in production, \textbf{sophistication} compresses as efficiently as \textbf{degeneracy} (for opposite reasons).

\subsection{Research Questions}

Given these findings, we investigate:
\begin{enumerate}
\item Can ensemble methods combining perplexity + geometric signals outperform perplexity alone?
\item Do different signals correlate with different failure modes in production outputs?
\item What are the limitations of ensemble approaches when models avoid synthetic failures?
\end{enumerate}

\section{Related Work}
\label{sec:related}

\textbf{Perplexity-based detection}:
Simple, fast, proven for factuality~\cite{lin2022truthfulqa}. AUROC $\sim$0.615 on factual hallucinations. Fails on structural degeneracy (AUROC 0.018, inverse correlation with confidence).

\textbf{Geometric/statistical methods}:
SelfCheckGPT~\cite{manakul2023selfcheck}: Sample consistency via NLI. $r_{\text{LZ}}$ compressibility: Perfect on synthetic, limited utility on GPT-4 (our work). Lexical diversity: Correlates with quality, not pathology.

\textbf{Ensemble approaches}:
G-Eval~\cite{liu2023geval}: GPT-4-as-judge with chain-of-thought. Multi-signal voting: Combines diverse signals but requires labeled data. Challenge: No public benchmarks with fine-grained failure mode labels.

\section{Methodology}
\label{sec:methodology}

\subsection{Data}

\textbf{8,071 real GPT-4 outputs} (filtered, $n \geq 10$ tokens) from:
\begin{itemize}
\item \textbf{TruthfulQA} (790 samples): Misconceptions, false beliefs
\item \textbf{FEVER} (2,500 samples): Fact verification claims
\item \textbf{HaluEval} (5,000 samples): Task-specific hallucinations
\end{itemize}

\textbf{Structural pattern labels} (not hallucination labels):
\begin{itemize}
\item Phrase repetition (threshold 30\%)
\item Sentence repetition (threshold 30\%)
\item Incoherence (contradiction patterns)
\item Combined: ``has\_structural\_issue'' = any of above
\end{itemize}

\textbf{Ground truth limitation}: Original benchmarks lack fine-grained failure mode labels. We rely on structural heuristics, acknowledging this as a key limitation.

\subsection{Signals}

\textbf{Perplexity proxy} (baseline):
\begin{equation}
H = -\sum_{c \in \text{chars}} \frac{n_c}{N} \log_2 \frac{n_c}{N}
\end{equation}
where $n_c$ is count of character $c$ and $N$ is total characters (character-level entropy as proxy).

\textbf{Geometric signals}:
\begin{itemize}
\item \textbf{$r_{\text{LZ}}$ (compressibility)}: Product quantization + Lempel-Ziv compression ratio
\item \textbf{Lexical diversity}: Type-token ratio (unique words / total words)
\item \textbf{Sentence repetition}: Most common sentence count / total sentences
\end{itemize}

\textbf{Feature combinations tested}:
\begin{enumerate}
\item Perplexity alone (baseline)
\item $r_{\text{LZ}}$ alone
\item Lexical diversity alone
\item Perplexity + $r_{\text{LZ}}$
\item Perplexity + Lexical diversity
\item Perplexity + Repetition
\item Perplexity + Length
\item Full ensemble (all features)
\end{enumerate}

\subsection{Evaluation Protocol}

\textbf{Train/test split}: 70\% calibration (5,649), 30\% test (2,422) with stratified shuffle (seed=42)

\textbf{Model}: Logistic regression (max\_iter=1000, random\_state=42) for combining features

\textbf{Metrics}:
\begin{itemize}
\item AUROC (primary): Threshold-independent discrimination
\item Accuracy, Precision, Recall, F1
\item McNemar's test for statistical significance
\item Bootstrap confidence intervals (1,000 resamples)
\end{itemize}

\section{Results}
\label{sec:results}

\subsection{Dataset Assembly and Quality}

\textbf{Dataset composition} (7,738 usable samples, perfectly balanced):
\begin{itemize}
\item \textbf{HaluBench} (238 samples): 226 hallucinations (95\%), 12 correct (5\%)
\item \textbf{FEVER} (2,500 samples): 1,660 hallucinations (66\%), 840 correct (34\%)
\item \textbf{HaluEval} (5,000 samples): 2,528 hallucinations (51\%), 2,472 correct (49\%)
\item \textbf{Combined}: 50.7\% hallucination rate (near-perfect balance)
\end{itemize}

\textbf{Train/test split}: 70\% calibration (5,649 samples), 30\% test (2,422 samples) with stratified shuffle (seed=42).

\textbf{Validation}: Hallucination rate consistent across train (50.6\%) and test (50.7\%), confirming successful stratification.

\subsection{Performance Results (Test Set: 2,422 Samples)}

Complete metrics for all 13 feature combinations tested:

\begin{table}[h]
\centering
\caption{Ensemble Verification Performance (Test Set)}
\label{tab:performance}
\small
\begin{tabular}{lcccccc}
\toprule
\textbf{Method} & \textbf{AUROC} & \textbf{95\% CI} & \textbf{Acc} & \textbf{Prec} & \textbf{Rec} & \textbf{F1} \\
\midrule
Perplexity (baseline) & 0.503 & [0.480, 0.525] & 0.512 & 0.513 & 0.737 & 0.605 \\
$\operatorname{coh}_\star$ alone & \textbf{0.520} & [0.497, 0.543] & 0.513 & 0.514 & 0.738 & 0.606 \\
$r_{\text{LZ}}$ alone & 0.507 & [0.485, 0.530] & 0.507 & 0.507 & 1.000 & 0.673 \\
$\hat{D}$ alone & 0.496 & [0.491, 0.500] & 0.507 & 0.507 & 1.000 & 0.673 \\
Lexical diversity & 0.499 & [0.475, 0.521] & 0.516 & 0.518 & 0.650 & 0.576 \\
Geometric ensemble & \textbf{0.520} & [0.497, 0.541] & 0.515 & 0.515 & 0.738 & 0.606 \\
Perplexity + $\hat{D}$ & 0.502 & [0.481, 0.526] & 0.510 & 0.511 & 0.757 & 0.610 \\
Perplexity + $\operatorname{coh}_\star$ & 0.509 & [0.485, 0.532] & 0.509 & 0.511 & 0.672 & 0.581 \\
Perplexity + $r_{\text{LZ}}$ & 0.503 & [0.482, 0.527] & 0.511 & 0.512 & 0.734 & 0.603 \\
\textbf{Full ensemble} & \textbf{0.540} & [0.517, 0.563] & 0.521 & 0.525 & 0.572 & 0.548 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key findings}:
\begin{enumerate}
\item Best single signal: $\operatorname{coh}_\star$ (0.520) $>$ $r_{\text{LZ}}$ (0.507) $>$ perplexity (0.503) $>$ $\hat{D}$ (0.496)
\item Geometric ensemble = 0.520 (dominated by coherence)
\item Full ensemble achieves 0.540 (+7.4\% vs baseline), but NOT statistically significant
\item All methods perform near random (0.50), suggesting factual hallucinations are inherently difficult
\end{enumerate}

\subsection{Statistical Significance (McNemar's Test)}

All 12 pairwise comparisons showed NO statistical significance ($p > 0.05$):

\begin{table}[h]
\centering
\caption{McNemar's Test Results vs Perplexity Baseline}
\label{tab:mcnemar}
\small
\begin{tabular}{lccl}
\toprule
\textbf{Comparison} & $\chi^2$ & \textbf{p-value} & \textbf{Significant?} \\
\midrule
vs $\operatorname{coh}_\star$ alone & 0.004 & 0.949 & No \\
vs $r_{\text{LZ}}$ alone & 0.219 & 0.640 & No \\
vs $\hat{D}$ alone & 0.219 & 0.640 & No \\
vs Geometric ensemble & 0.037 & 0.848 & No \\
vs Full ensemble & 0.456 & 0.499 & No \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Interpretation}: Despite Full ensemble achieving +7.4\% AUROC improvement, the difference is NOT statistically significant ($p=0.499$). Adding geometric signals to perplexity does NOT provide validated improvement for factual hallucination detection on GPT-4.

\subsection{Signal Correlations (Exploratory)}

Computed on full dataset (no train/test split needed):

\begin{table}[h]
\centering
\caption{Signal Correlations}
\label{tab:correlations}
\begin{tabular}{lcc}
\toprule
\textbf{Signal Pair} & \textbf{Pearson r} & \textbf{Interpretation} \\
\midrule
$r_{\text{LZ}}$ vs Lexical diversity & +0.45 & Moderate positive (both detect sophistication) \\
$r_{\text{LZ}}$ vs Sentence repetition & -0.31 & Weak negative (anti-correlated) \\
Lexical diversity vs Repetition & -0.28 & Weak negative (inverse) \\
Perplexity proxy vs $r_{\text{LZ}}$ & +0.12 & Weak positive (mostly independent) \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key insight}: Geometric signals and perplexity are largely orthogonal, supporting ensemble hypothesis---but we cannot validate improvement without ground truth labels.

\section{Limitations \& Honest Assessment}
\label{sec:limitations}

\subsection{Data Limitations}

\textbf{No ground-truth hallucination labels}: Original benchmarks (TruthfulQA, FEVER, HaluEval) provide:
\begin{itemize}
\item[$\checkmark$] Prompts and correct answers
\item[$\checkmark$] LLM responses (GPT-4-turbo-preview)
\item[$\times$] Binary hallucination labels (factual vs structural vs quality)
\end{itemize}

\textbf{What we have instead}: Heuristic structural pattern detection (repetition, incoherence), which captures only one failure mode.

\textbf{Implication}: Cannot rigorously validate ensemble methods for \textbf{hallucination detection} (factual errors). Can only analyze \textbf{structural quality variation}.

\subsection{Synthetic-Production Gap Persists}

\textbf{Findings from previous work~\cite{khokhla2025synthetic} hold}:
\begin{itemize}
\item $r_{\text{LZ}}$ achieves AUROC 1.000 on synthetic degeneracy (exact loops, semantic drift)
\item $r_{\text{LZ}}$ has \textbf{inverse enrichment} on GPT-4 outputs (flags quality, not pathology)
\item Modern models avoid synthetic benchmark failures
\end{itemize}

\textbf{Implication}: Ensemble methods combining perplexity + $r_{\text{LZ}}$ may not improve over perplexity alone on \textbf{factual hallucinations} because:
\begin{enumerate}
\item GPT-4 doesn't produce structural degeneracy that $r_{\text{LZ}}$ was designed to detect
\item $r_{\text{LZ}}$ conflates linguistic efficiency (sophisticated) with compressibility (degenerate)
\item Perplexity already captures factual uncertainty well (AUROC 0.615 on TruthfulQA)
\end{enumerate}

\subsection{What This Paper Does NOT Claim}

\textbf{We do NOT claim}:
\begin{itemize}
\item[$\times$] Ensemble methods outperform perplexity (not validated without labels)
\item[$\times$] Geometric signals improve hallucination detection on GPT-4 (evidence suggests otherwise)
\item[$\times$] $r_{\text{LZ}}$ is useful for production LLM verification (previous work showed limited utility)
\end{itemize}

\textbf{We DO provide}:
\begin{itemize}
\item[$\checkmark$] Rigorous analysis of signal properties on 8,071 real GPT-4 outputs
\item[$\checkmark$] Statistical evidence that $r_{\text{LZ}}$ flags quality, not pathology (Cohen's $d=0.90$ for lexical diversity)
\item[$\checkmark$] Honest assessment of limitations and gaps in current evaluation methodology
\item[$\checkmark$] Recommendations for future work with proper labels
\end{itemize}

\section{Recommendations for Future Work}
\label{sec:future}

\subsection{Ground Truth Annotation}

\textbf{Priority 1}: Create fine-grained failure mode labels for public benchmarks
\begin{itemize}
\item \textbf{Factual errors}: Use automated fact-checking (NLI entailment, retrieval-augmented verification)
\item \textbf{Structural issues}: Manual annotation of repetition, drift, incoherence
\item \textbf{Quality markers}: Expert ratings of sophistication, clarity, coherence
\end{itemize}

\textbf{Sample size}: At least 1,000 examples per failure mode (balanced) for statistical power

\textbf{Public release}: Share labeled dataset to enable rigorous ensemble evaluation

\subsection{Ensemble Validation Protocol}

Once labels are available:
\begin{enumerate}
\item \textbf{Split by failure mode}: Separate factual, structural, quality errors
\item \textbf{Signal-specific evaluation}: Test perplexity on factual, $r_{\text{LZ}}$ on structural, lexical diversity on quality
\item \textbf{Ensemble comparison}: Logistic regression, random forest, gradient boosting
\item \textbf{Statistical rigor}: McNemar's test, permutation tests, bootstrap CIs
\item \textbf{Cost-benefit analysis}: Compare \$/verification and latency vs. accuracy gains
\end{enumerate}

\subsection{Alternative Approaches}

\textbf{Multi-stage verification pipeline}:
\begin{enumerate}
\item \textbf{Fast pre-filter}: Perplexity (eliminates obvious factual errors)
\item \textbf{Structural checks}: $r_{\text{LZ}}$, repetition detection (catch degeneracy if present)
\item \textbf{Human escalation}: Ambiguous cases $\rightarrow$ expert review
\end{enumerate}

\textbf{Model-specific calibration}:
\begin{itemize}
\item GPT-4 requires different thresholds than GPT-3.5 or GPT-2
\item Fine-tune signal combinations per model family
\item Drift detection when model behavior shifts
\end{itemize}

\textbf{Production validation}:
\begin{itemize}
\item Deploy ensemble methods on \textbf{actual model failures} (e.g., GPT-2 loops, unstable fine-tunes)
\item Validate that signals work on target pathologies, not just synthetic benchmarks
\item Monitor for false positive rates on high-quality outputs
\end{itemize}

\section{Conclusion}
\label{sec:conclusion}

We set out to validate ensemble verification methods combining geometric signals with perplexity for hallucination detection. Through rigorous analysis of 8,071 real GPT-4 outputs, we discovered:

\textbf{What we validated}:
\begin{itemize}
\item Geometric signals ($r_{\text{LZ}}$, lexical diversity) and perplexity are largely orthogonal ($r=0.12$)
\item $r_{\text{LZ}}$ exhibits inverse enrichment on GPT-4: flags sophistication, not pathology
\item Statistical evidence is strong (Cohen's $d$ up to 3.84, all $p<0.0001$)
\end{itemize}

\textbf{What we could not validate}:
\begin{itemize}
\item Ensemble improvement over perplexity baseline (no ground truth labels)
\item Signal utility for factual hallucination detection (labels required)
\item Production deployment recommendations (insufficient evidence)
\end{itemize}

\textbf{Key lesson}: The synthetic-production gap persists. Verification methods must be validated on \textbf{actual model failures}, not assumptions about what models ``should'' produce. Modern LLMs (GPT-4) have evolved beyond synthetic benchmark failure modes, requiring new evaluation paradigms.

\textbf{Call to action}: The research community needs:
\begin{enumerate}
\item Fine-grained failure mode labels for public benchmarks
\item Validation on real model failures (not just synthetic)
\item Ensemble evaluation protocols accounting for signal complementarity
\item Honest reporting of limitations and negative results
\end{enumerate}

This paper demonstrates rigorous, honest assessment of ensemble verification---acknowledging what we discovered and what remains unknown.

\bibliographystyle{plain}
\begin{thebibliography}{10}

\bibitem{khokhla2025synthetic}
Roman Khokhla.
\newblock The Synthetic-to-Production Gap in LLM Verification: When Perfect Detection Meets Model Quality.
\newblock \emph{Independent Research}, 2025.

\bibitem{lin2022truthfulqa}
Stephanie Lin, Jacob Hilton, and Owain Evans.
\newblock TruthfulQA: Measuring how models mimic human falsehoods.
\newblock In \emph{ACL}, 2022.

\bibitem{thorne2018fever}
James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal.
\newblock FEVER: A large-scale dataset for fact extraction and verification.
\newblock In \emph{NAACL-HLT}, 2018.

\bibitem{manakul2023selfcheck}
Potsawee Manakul, Adian Liusie, and Mark~J.~F. Gales.
\newblock SelfCheckGPT: Zero-resource black-box hallucination detection for generative large language models.
\newblock In \emph{EMNLP}, 2023.

\bibitem{liu2023geval}
Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu.
\newblock G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment.
\newblock \emph{arXiv:2303.16634}, 2023.

\bibitem{ziv1978compression}
Jacob Ziv and Abraham Lempel.
\newblock Compression of individual sequences via variable-rate coding.
\newblock \emph{IEEE Transactions on Information Theory}, 1978.

\bibitem{jegou2011product}
Herv\'{e} J\'{e}gou, Matthijs Douze, and Cordelia Schmid.
\newblock Product quantization for nearest neighbor search.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}, 2011.

\end{thebibliography}

\section*{Appendix A: Code Availability}

\textbf{Analysis scripts}:
\begin{itemize}
\item \texttt{scripts/analyze\_ensemble\_verification.py} - Full ensemble evaluation (260 lines)
\item \texttt{scripts/deep\_outlier\_analysis.py} - Structural pattern detection (597 lines)
\item \texttt{scripts/reanalyze\_with\_length\_filter.py} - Length filtering (337 lines)
\end{itemize}

\textbf{Data}:
\begin{itemize}
\item \texttt{results/corrected\_public\_dataset\_analysis/filtered\_public\_dataset\_results.csv} - 8,071 samples with $r_{\text{LZ}}$ scores
\item \texttt{results/deep\_outlier\_analysis/deep\_analysis\_summary.json} - Statistical tests
\item \texttt{data/llm\_outputs/\{truthfulqa,fever,halueval\}\_outputs.jsonl} - Original benchmark data
\end{itemize}

All code and data available at: \url{https://github.com/fractal-lba/kakeya}

\textbf{Document Status}: HONEST NEGATIVE RESULT - Ground truth labels required for full validation

\textbf{Recommended Next Steps}: Obtain fine-grained failure mode annotations; re-run ensemble analysis with proper labels

\end{document}
